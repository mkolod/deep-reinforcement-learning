{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Tennis_Linux/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.10000000149011612\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.09000000171363354\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ou_noise import OUNoise\n",
    "from utils import hard_update, soft_update\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "    def __init__(self, \n",
    "                 in_actor,\n",
    "                 hidden_in_actor=256, \n",
    "                 hidden_out_actor=256,\n",
    "                 out_actor=2,\n",
    "                 leak=0.01, \n",
    "                 seed=42):\n",
    "        \"\"\" Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            in_actor (int): Size of state tensor\n",
    "            hidden_in_actor (int): Number of hidden units in FC layer 1\n",
    "            hidden_out_actor (int): Number of hidden units in FC layer 2           \n",
    "            out_actor (int): Size of action tensor\n",
    "            seed (int): Random seed                    \n",
    "            leak (float): the leak rate for leaky ReLU, i.e. the alpha in (x < 0) * alpha * x + (x >= 0) * x\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.leak = leak\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_actor, hidden_in_actor)\n",
    "        self.fc2 = nn.Linear(hidden_in_actor, hidden_out_actor)\n",
    "        self.fc3 = nn.Linear(hidden_out_actor, out_actor)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(in_actor)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize FC layers followed by leaky ReLU using Kaiming He's (2015) approach.\n",
    "        Source: https://arxiv.org/pdf/1502.01852v1.pdf\n",
    "        For more info see here:\n",
    "            https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/\n",
    "        \"\"\"\n",
    "        torch.nn.init.kaiming_normal_(\n",
    "            self.fc1.weight.data,\n",
    "            a=self.leak, \n",
    "            nonlinearity='leaky_relu',\n",
    "            mode='fan_in')\n",
    "        torch.nn.init.kaiming_normal_(self.fc2.weight.data,\n",
    "                                      a=self.leak,\n",
    "                                      nonlinearity='leaky_relu',\n",
    "                                      mode='fan_in')\n",
    "        torch.nn.init.uniform_(self.fc3.weight.data,\n",
    "                               -3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "#         state = self.bn(state)\n",
    "        x = F.leaky_relu(self.fc1(state), negative_slope=self.leak)\n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope=self.leak)\n",
    "        x =  torch.tanh(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "    def __init__(self,\n",
    "                 in_critic,\n",
    "                 hidden_in_critic=256,\n",
    "                 hidden_out_critic=256,\n",
    "                 out_critic=1, \n",
    "                 leak=0.01, \n",
    "                 seed=42):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            in_critic (int): Dimension of each state            \n",
    "            hidden_in_critic (int): Number of nodes in the first hidden layer\n",
    "            hidden_out_critic (int): Number of nodes in the second hidden layer\n",
    "            out_critic (int): Dimension of each action\n",
    "            leak (float): Leakiness of leaky ReLU\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.leak = leak\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.bn = nn.BatchNorm1d(in_critic)\n",
    "        self.fc1 = nn.Linear(in_critic, hidden_in_critic)\n",
    "        self.fc2 = nn.Linear(hidden_in_critic, hidden_out_critic)\n",
    "        self.fc3 = nn.Linear(hidden_out_critic, out_critic)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize FC layers followed by leaky ReLU using Kaiming He's (2015) approach.\n",
    "        Source: https://arxiv.org/pdf/1502.01852v1.pdf\n",
    "        For more info see here:\n",
    "            https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/\n",
    "        \"\"\"\n",
    "        torch.nn.init.kaiming_normal_(self.fc1.weight.data, a=self.leak,\n",
    "                                      nonlinearity='leaky_relu', mode='fan_in')\n",
    "        torch.nn.init.kaiming_normal_(self.fc2.weight.data, a=self.leak,\n",
    "                                      nonlinearity='leaky_relu', mode='fan_in')\n",
    "        torch.nn.init.uniform_(self.fc3.weight.data, -3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\" Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "#         state = self.bn(state)\n",
    "        x = F.leaky_relu(self.fc1(state), negative_slope=self.leak)\n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope=self.leak)\n",
    "        x =  self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, \n",
    "                 in_actor, \n",
    "                 hidden_in_actor,\n",
    "                 hidden_out_actor,\n",
    "                 out_actor, \n",
    "                 in_critic,\n",
    "                 hidden_in_critic,\n",
    "                 hidden_out_critic,\n",
    "                 out_critic,\n",
    "                 device,\n",
    "                 relu_leak=1e-2,\n",
    "                 seed=42,\n",
    "                 lr_actor=1.0e-2, \n",
    "                 lr_critic=1.0e-2):\n",
    "        super(DDPGAgent, self).__init__()\n",
    "        self.device=device\n",
    "        self.actor = Actor(\n",
    "            in_actor,\n",
    "            hidden_in_actor, \n",
    "            hidden_out_actor, \n",
    "            out_actor, \n",
    "            relu_leak, \n",
    "            seed).to(device)\n",
    "        self.critic = Critic(\n",
    "            in_critic,\n",
    "            hidden_in_critic,\n",
    "            hidden_out_critic, \n",
    "            out_critic, \n",
    "            relu_leak, \n",
    "            seed).to(device)\n",
    "        self.target_actor = Actor(\n",
    "            in_actor, \n",
    "            hidden_in_actor,\n",
    "            hidden_out_actor,\n",
    "            out_actor, \n",
    "            relu_leak,\n",
    "            seed).to(device)\n",
    "        self.target_critic = Critic(\n",
    "            in_critic, \n",
    "            hidden_in_critic,\n",
    "            hidden_out_critic,\n",
    "            out_critic, \n",
    "            relu_leak, \n",
    "            seed).to(device)\n",
    "\n",
    "        self.noise = OUNoise(out_actor, seed)\n",
    "        \n",
    "        # initialize targets same as original networks\n",
    "        hard_update(self.target_actor, self.actor)\n",
    "        hard_update(self.target_critic, self.critic)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_critic, weight_decay=1.e-5)   \n",
    "        \n",
    "    def act(self, states, noise_mul=0.0):\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor(states).cpu().data.numpy()\n",
    "        actions += self.noise.sample() * noise_mul\n",
    "        return np.clip(actions, -1, 1)        \n",
    "    \n",
    "    def target_act(self, states, noise_mul=0.0):\n",
    "        self.target_actor.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.target_actor(states).cpu().data.numpy()\n",
    "        actions += self.noise.sample() * noise_mul\n",
    "        return np.clip(actions, -1, 1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPGAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_agents,\n",
    "        in_actor, \n",
    "        hidden_in_actor,\n",
    "        hidden_out_actor,\n",
    "        out_actor, \n",
    "        hidden_in_critic,\n",
    "        hidden_out_critic,\n",
    "        out_critic,\n",
    "        device,\n",
    "        relu_leak=1e-2,\n",
    "        seed=42,\n",
    "        lr_actor=1.0e-2, \n",
    "        lr_critic=1.0e-2,\n",
    "        replay_buffer_size=int(1e6),\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        tau=1e-3,\n",
    "        update_every=1,\n",
    "        num_updates=1\n",
    "    ):        \n",
    "        in_critic = in_actor * num_agents + out_actor\n",
    "        self.agents = []\n",
    "        self.num_agents = num_agents\n",
    "        self.out_actor = out_actor\n",
    "        self.replay_buffer = ReplayBuffer(num_agents * out_actor, replay_buffer_size, batch_size, seed, device)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "        self.steps = 0\n",
    "        self.update_every = update_every\n",
    "        self.num_updates = num_updates\n",
    "        self.noise_scale = 1.0\n",
    "        for _ in range(num_agents):\n",
    "            self.agents.append(\n",
    "                DDPGAgent(\n",
    "                    in_actor, hidden_in_actor, hidden_out_actor, out_actor,\n",
    "                    in_critic, hidden_in_critic, hidden_out_critic, out_critic, \n",
    "                    device, relu_leak, seed, lr_actor, lr_critic)\n",
    "            )          \n",
    "\n",
    "    def act(self, states, noise_mul=0.0):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        actions = np.zeros((self.num_agents, self.out_actor))\n",
    "        for agent_idx, state in enumerate(states):\n",
    "            state = torch.from_numpy(state).float().to(self.device)\n",
    "            if state.dim() < 2:\n",
    "                state = state.unsqueeze(0)\n",
    "            curr_agent = self.agents[agent_idx]\n",
    "            action = curr_agent.act(state, noise_mul)\n",
    "            actions[agent_idx, :] = action\n",
    "        return actions\n",
    "            \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        self.steps += 1\n",
    "        self.replay_buffer.add(states, actions, rewards, next_states, dones)\n",
    "        print(self.replay_buffer.memory[0][0])\n",
    "        print(self.replay_buffer.memory[0][1])\n",
    "        print(self.replay_buffer.memory[0][2])\n",
    "        print(self.replay_buffer.memory[0][3])\n",
    "        print(self.replay_buffer.memory[0][4])\n",
    "\n",
    "        if (len(self.replay_buffer) > self.batch_size) and (self.steps % self.update_every == 0):\n",
    "            # reset steps so we don't overflow at some point\n",
    "            self.steps = 0\n",
    "            print(\"learning\")\n",
    "            for _ in range(self.num_updates):\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "                \n",
    "    def learn(self, experiences, gamma):\n",
    "        \n",
    "        all_agents_next_actions = []\n",
    "        all_agents_actions = []\n",
    "        \n",
    "        for agent_id, agent in enumerate(self.agents):\n",
    "            \n",
    "            all_states, all_actions, all_rewards, all_next_states, all_dones = experiences\n",
    "            agent_idx = torch.tensor([agent_id]).to(self.device)\n",
    "            states = torch.index_select(all_states, 1, agent_idx)\n",
    "            actions = torch.index_select(all_actions, 1, agent_idx)\n",
    "            rewards = torch.index_select(all_rewards, 1, agent_idx)\n",
    "            next_states = torch.index_select(all_next_states, 1, agent_idx)\n",
    "            dones = torch.index_select(all_dones, 1, agent_idx)\n",
    "            \n",
    "            # Critic (aware of all agents' observations)\n",
    "            agent.critic_optimizer.zero_grad()\n",
    "            target_actions = agent.target_act(next_states, self.noise_scale)\n",
    "            target_actions = torch.from_numpy(target_actions).to(self.device)\n",
    "#             target_actions = torch.cat(target_actions, dim=1)        \n",
    "            print(\"all_next_states.shape={}\".format(all_next_states.shape))\n",
    "            all_next_states = all_next_states.reshape(self.batch_size, -1)\n",
    "            print(\"all_next_states.shape={}\".format(all_next_states.shape))\n",
    "            target_actions = target_actions.squeeze()\n",
    "            print(\"target_actions.shape={}\".format(target_actions.shape))\n",
    "            target_critic_input = torch.cat((all_next_states.reshape(self.batch_size, -1),target_actions), dim=1)\n",
    "            with torch.no_grad():\n",
    "                q_next = agent.target_critic(target_critic_input)\n",
    "            y = rewards[agent_id].view(-1, 1) + self.gamma * q_next * (1 - done[agent_number].view(-1, 1))\n",
    "            action = torch.cat(action, dim=1)\n",
    "            critic_input = torch.cat((all_states.t(), action), dim=1).to(device)\n",
    "            q = agent.critic(critic_input)\n",
    "            huber_loss = torch.nn.SmoothL1Loss()\n",
    "            critic_loss = huber_loss(q, y.detach())\n",
    "            critic_loss.backward()\n",
    "            agent.critic_optimizer.step()\n",
    "            \n",
    "            # Actor\n",
    "            \n",
    "            agent.actor_optimizer.zero_grad()\n",
    "            \n",
    "            q_input = [ self.maddpg_agent[i].actor(ob) if i == agent_number \\\n",
    "                       else self.maddpg_agent[i].actor(ob).detach()\n",
    "                       for i, ob in enumerate(obs) ]                        \n",
    "            \n",
    "            q_input = torch.cat(q_input, dim=1)\n",
    "            q_input2 = torch.cat((obs_full.t(), q_input), dim=1)\n",
    "            \n",
    "            actor_loss = -agent.critic(q_input2).mean()\n",
    "            actor_loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(agent.actor.parameters(),0.5)\n",
    "            agent.actor_optimizer.step()\n",
    "            \n",
    "            al = actor_loss.cpu().detach().item()\n",
    "            cl = critic_loss.cpu().detach().item()\n",
    "            print(\"Agent {} losses: critic loss: {}, actor loss: {}\".format(agent_id, cl, al))        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def save_agents(self):\n",
    "        raise NotImplementedError(\"Saving is not implemented yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 2\n",
    "\n",
    "in_actor = 24\n",
    "hidden_in_actor = 128\n",
    "hidden_out_actor = 128\n",
    "out_actor = 2\n",
    "\n",
    "hidden_in_critic = 128\n",
    "hidden_out_critic = 1\n",
    "out_critic = 1\n",
    "\n",
    "lr_actor = 1e-3\n",
    "lr_critic = 1e-3 \n",
    "relu_leak = 1e-2\n",
    "seed = 42\n",
    "\n",
    "gpu_id = 0\n",
    "use_gpu = True\n",
    "device = 'cuda:{}'.format(gpu_id) if use_gpu else 'cpu'\n",
    "\n",
    "replay_buffer_size = int(1e6)\n",
    "batch_size = 16\n",
    "gamma = 0.99\n",
    "tau = 1e-3\n",
    "update_every = 1\n",
    "num_updates = 1\n",
    "\n",
    "meta_agent = MADDPGAgent(  \n",
    "    num_agents,\n",
    "    in_actor, \n",
    "    hidden_in_actor,\n",
    "    hidden_out_actor,\n",
    "    out_actor, \n",
    "    hidden_in_critic,\n",
    "    hidden_out_critic,\n",
    "    out_critic,\n",
    "    device,\n",
    "    relu_leak,\n",
    "    seed,\n",
    "    lr_actor, \n",
    "    lr_critic,\n",
    "    replay_buffer_size,\n",
    "    batch_size,\n",
    "    gamma,\n",
    "    tau,\n",
    "    update_every,\n",
    "    num_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04495187 -0.01474663]\n",
      " [ 0.03207987  0.00519489]]\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "states = env_info.vector_observations\n",
    "\n",
    "actions = meta_agent.act(states)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03867292 -0.01440356]\n",
      " [ 0.02226311 -0.00378614]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "Score (max over agents) from episode 0: 0.0\n",
      "[[ 0.06748401 -0.01024526]\n",
      " [ 0.06312688  0.01794526]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[ 0.03383572 -0.06505661]\n",
      " [ 0.09451949 -0.02171843]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[-0.02698239 -0.12278403]\n",
      " [ 0.12252057 -0.01094019]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[ 0.00170265 -0.06681053]\n",
      " [ 0.10703479 -0.02011374]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[-0.00152238 -0.0286346 ]\n",
      " [ 0.10133068 -0.01003142]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[-0.01320504 -0.05670789]\n",
      " [ 0.10642113 -0.02310126]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[-0.01070272 -0.04614926]\n",
      " [ 0.10323194 -0.01813924]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[-0.00656767 -0.03425324]\n",
      " [ 0.10067419 -0.01641295]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[ 0.00016248 -0.02857269]\n",
      " [ 0.09808496 -0.01588297]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[ 0.00830964 -0.02736369]\n",
      " [ 0.09196635 -0.0152539 ]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[ 0.01457684 -0.0285695 ]\n",
      " [ 0.08266776 -0.01305896]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[ 0.02051933 -0.03092875]\n",
      " [ 0.06963418 -0.00725918]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[ 0.02257776 -0.03310828]\n",
      " [ 0.0582553   0.00045973]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[ 0.01530539 -0.03566536]\n",
      " [ 0.04478132  0.00824271]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "Score (max over agents) from episode 1: 0.0\n",
      "[[ 0.06544513  0.02130679]\n",
      " [ 0.06777356 -0.01184761]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "[[ 0.09608973 -0.01729885]\n",
      " [ 0.0325488  -0.06609116]]\n",
      "states.shape=(2, 24)\n",
      "actions.shape=(2, 2)\n",
      "rewards.shape=2\n",
      "next_states.shape=(2, 24)\n",
      "dones.shape=2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14420891 -1.5\n",
      "  -0.          0.          7.5460577  -2.30871558 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.74939442 -1.5\n",
      "   0.          0.         -7.5460577  -2.30871558  0.          0.        ]]\n",
      "tensor([[ 0.0387, -0.0144],\n",
      "        [ 0.0223, -0.0038]], device='cuda:0', dtype=torch.float64)\n",
      "[0.0, -0.009999999776482582]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.59082413 -1.5\n",
      "  -0.          0.          6.52740669  5.97645617 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.50744295 -1.5\n",
      "   0.          0.         -6.52740669  5.97645617  0.          0.        ]]\n",
      "[True, True]\n",
      "learning\n",
      "all_next_states.shape=torch.Size([16, 2, 24])\n",
      "all_next_states.shape=torch.Size([16, 48])\n",
      "target_actions.shape=torch.Size([16, 2])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ddd521a8be39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dones.shape={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mmeta_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-1445296bc91e>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_updates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-1445296bc91e>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mq_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_critic_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_next\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_number\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mcritic_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'done' is not defined"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "num_steps_max = 1000\n",
    "\n",
    "print_every = 10\n",
    "\n",
    "for i in range(num_episodes):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "    for j in range(num_steps_max):\n",
    "        actions = meta_agent.act(states)\n",
    "        print(actions)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards        \n",
    "        dones = env_info.local_done\n",
    "        print(\"states.shape={}\".format(states.shape))\n",
    "        print(\"actions.shape={}\".format(actions.shape))\n",
    "        print(\"rewards.shape={}\".format(len(rewards)))\n",
    "        print(\"next_states.shape={}\".format(next_states.shape))\n",
    "        print(\"dones.shape={}\".format(len(dones)))\n",
    "        actions = torch.from_numpy(actions).to(meta_agent.device)\n",
    "        meta_agent.step(states, actions, rewards, next_states, dones)\n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        if np.any(dones):            \n",
    "            break\n",
    "            \n",
    "        if j > batch_size and j % print_every == 0:\n",
    "            print(\"episode {}, step {}: done\".format(i, j))\n",
    "            \n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
