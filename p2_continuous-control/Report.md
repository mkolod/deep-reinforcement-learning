## Report regarding training of DDPG agent to play Unity ML Agents' Reacher Environment


###  Problem Statement

This project implements the training and inference of Unity ML Agents'  [Reacher environment](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md). 

In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location, where the goal is keeping the arm in contact with the moving ball. In the above animation, the ball changes color from blue to green when the agent stays in contact with the ball. The goal of the agent is to maintain its position at the target location for as many time steps as possible. 

There are 2 versions of the Unity environment, one with a single environment and one with 20 environments, which can be used by 20 agents.

The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.

The environment is considered solved if the agent reaches a score of +30.0 in 100 consecutive episodes. For 20 agents, one needs to get the same score over 100 consecutive episodes, but averaged over all agents for each episode.

### Learning Algorithm

To solve the environment, I used the Deep Deterministic Policy Gradient DDPG algorithm (see [here](https://arxiv.org/pdf/1509.02971.pdf) for the paper introducing it). 

On the surface, DDPG looks like an actor-critic model, because we have two networks. Strictly speaking though, it's not an actor-critic model, though the authors said that it was. Thus, let's first refresh what an actor-critic model is about. 

Actor-critic models are not only based on two networks, they are based on two different reinforcement learning approaches. The actor network is normally a Monte Carlo-based, value function predicting model. The strength of such a network is that it's unbiased, because entire episodes are rolled out by collecting actual feedback from the environment. Unfortunately, they are also high variance, because many trajectories can pass through the same state. Also, rolling out entire episodes means that the actor is very data-inefficient. The critic is the opposite - it's a policy network, which uses a temporal difference (TD) approach, which means that only one step has to be simulated (the immediate interaction). The discounted future reward is taken from a prediction, but since we then use an estimate rather than a roll-out, a TD approach can have bias, even though it has reduced variance (because there's only one direct interaction with the environment). Combining the no-bias, high-variance actor and the low-bias, low-variance critic, we eventually get the actor which is both lower variance and has some initial guesses to work off of to speed up convergence. Of course, once the model is train, we discard the critic and just use the actor for inference, so inference is still efficient. Training may require more compute, but the convergence is quicker and more stable, so having the critic help the actor is a net win.

It was worth explaining traditional actor-critic models to compare DDPG against them. In a lot of ways, even though DDPG has two networks, it's not really an actor-critic in the traditional sense. It can be seen as an approximate DQN. The reason is that the "critic" in DDPG is used to approximate the maximizer over the Q values of the next state, and not as a learned baseline. 

The main problem with DQN agents is that it's not obvious how to use them for problems with continuous action spaces. When a DQN outputs the Q values for each state-action pair (or rather, for the current state and each action), it's easy to choose the best action using the argmax over those outputs. However, what would we do if we have a continuous output? We no longer can take the argmax, because it's not a discrete action space anymore - we have to guess what the max should be. 

DDPG solves the continuous action space problem. We use 2 neural networks. Even though DDPG is not really a "true" actor-critic model (because it doesn't combine Monte Carlo action-value network with a TD learning-based policy network), let's use that terminology for now instead of referring to network 1 and network 2. In DDPG, the actor predicts the optimal policy deterministically. This means the actor gives only the best action, rather than a probability distribution over actions. In other words, the actor predicts argmax<sub>a</sub>Q(s, a). Of course, it's not a discrete argmax because the actions are continuous, it's the action which is somewhere on the real line of continuous values that ends up maximizing Q. Let's call this $\mu(s, \theta_{\mu}$). 
The critic learns to evaluate the optimal value function based on the actor's best chosen action. We can write this as $Q(s, \mu(s, \theta_{\mu}); \theta_Q)$. To rephrase, we use the actor, which is an approximate maximizer, to calculate a new target value to train a new action-value function, which the critic provides.  This does sound like an approximate DQN. Another DQN-style feature is the use of the replay buffer. Another thing which is important to mention is the use of the soft update approach. In DQNs, there's a local network and a target network. The target is normally updated only once in a few hundred of thousand time steps, while the local actor is updated at each time step. The updates are then done to the target by copying the weights at those irregular intervals. The DDPG approach, and indeed one used in other modern architectures, is to make much smaller, but regular updates to the target by linearly interpolating between the target and the local actor. To make sure the updates are small, the weight attached to the contribution of the local actor is $0 <\tau<1$, with the value of $\tau$ being much closer to 0 than to 1, and with the weight attached to the last value of the target being $1-\tau$, which clearly then ends up being much closer to 1 than to 0.

Last but not least, if it wasn't obvious so far, DDPG is an off-policy, model-free algorithm. 
For more details, see the [DDPG paper](https://arxiv.org/pdf/1509.02971.pdf).


### Implementation, Model code and Hyperparameters
 
 I chose to use the 20-agent environment, because it results in faster training.
 
 The model can be found [here](https://github.com/mkolod/deep-reinforcement-learning/blob/master/p2_continuous-control/model.py). It was largely inspired by the DDPG model from the lessons, specifically the models used to solve OpenAI's [bipedal](https://github.com/mkolod/deep-reinforcement-learning/blob/master/ddpg-bipedal/model.py) and [pendulum](https://github.com/mkolod/deep-reinforcement-learning/blob/master/ddpg-pendulum/model.py) environments. Since the level of problem complexity wasn't drastically different, I thought it was good to start with similar architectures. 

One idea borrowed specifically from the bipedal model was to use Leaky ReLU as activation functions, instead of ReLU. Leaky ReLU has also found success in other model architectures such as DCGANs. The advantage over non-leaky ReLU activations is that leaky ReLUs solve a problem called "dying." If the dot product of the input to a layer preceding the activation and that layer's weights is negative, ReLU produces a value of 0. This means that there is no gradient to learn from, because the gradient of max(x, 0) is 0 if the forward pass output was 0. This makes learning hard, especially for notoriously "tricky to train" problems such as GANs and RL models.

One improvement I made to the model is weight initialization. As lots of papers demonstrated in the past, inappropriate initialization leads to poor learning. For layers with tanh and softsign activations, we can use the "Xavier" initialization by Glorot and Bengio 2010 ([here's
(http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) the link to the paper). For ReLU and leaky ReLU, Kaiming He (2015) discovered another initialization which is optimized for convergence of models using rectifiers. The link to He's paper is [here](https://arxiv.org/pdf/1502.01852v1.pdf). A nice discussion why that's the right thing to do can be found [here](https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/).  The other choice one needs to make is whether the initialization is based on the normal or uniform distributions. I found the normal distribution to work quite well (see [here](https://github.com/mkolod/deep-reinforcement-learning/blob/master/p2_continuous-control/model.py#L38:L46) for the actor and [here](https://github.com/mkolod/deep-reinforcement-learning/blob/master/p2_continuous-control/model.py#L88:L92) for the critic). For the output layer of the actor which uses tanh, and the critic output which uses linear output, I tried alternative initializations, but the default narrow-range ([-1e-3, 1e-3]) uniform initialization worked the best. 

Note that compared to the bipedal and pendulum environments, the hidden states are smaller. For example, in bipedal, fully-connected layers have (256, 256) hidden states in the actor, and critic sizes were (256, 256, 128). In the case of pendulum, the hidden states were (400, 300) for the actor, and (400, 300) for the critic. For solving the Reacher environment, I found that hidden state sizes (256, 128) for the actor and (256, 128, 64) were good enough for the critic. Larger state sizes would either result in very slow learning, or poor convergence even after a very long time. One could use dropout or say L2 regularization to help with overfitting, but the problem then would be with a large, slow-at-inference-time network. If it's possible to make a network smaller from the beginning, that's always a good thing (less memory and compute). In fact, these smaller actor and critic trained rather quickly in terms of the number of episodes, and the training was stable over time (see the reward plot below). 

Just as described in the explanation of DDPG, the actor gives us $\mu(s, \theta_{\mu}$, which is then used by the critic to generate $Q(s, \mu(s, \theta_{\mu}); \theta_Q)$. The way this interaction is set up in the critic is by concatenating the transformed state and the output of the actor, which can be found [here](https://github.com/mkolod/deep-reinforcement-learning/blob/master/p2_continuous-control/model.py#L98). The question that one might ask is how that benefits the actor as the critic learns. Since the output of the actor is fed into the critic and is parameterized by the actor's weights, as the critic undergoes backpropagation, the backprop goes through the entire critic network, which means backpropagating through the actor which feeds into it.

The agent is very similar to the bipedal and pendulum DDPG agents, but with some small differences. For one, I parameterized the agent with lots of knobs that can be controlled from the client code, so that they are not hard-coded as constants in the file (see [here](https://github.com/mkolod/deep-reinforcement-learning/blob/master/p2_continuous-control/ddpg_agent.py#L13:L27)). You can also note the default hyperparameters which ended up being used to train a successful agent. Most of them are the same as for the bipedal and pendulum environments. The main difference is that instead of updating at every step, I ran the learn() method every 10 calls to agent.step(). The main thing that ensures is that we update the replay buffer a bit before making another learning step.  However, once we update the replay buffer a bit, I call for 20 updates. This happens to coincide with the number of agents (20), but doesn't necessarily need to. Also, care needs to be taken while running with multiple agent. When running with 20 agents, the Ornstein-Uhlenbeck noise needs to be added to all of the agents' actions, not just to one. Forgetting to do that results in very poor training in a multi-agent environment for the same reason that not adding noise to one agent's actions in a single-agent environment would. Other than that, the agent implementation is basically the same as the bipedal and pendulum implementations. 

One last thing to note is that because it's hard to deal with an environment that resulted in a "done" (end of episode) state once that occurs, I opted for ending all 20 agents' interaction once one of them reaches the end of the episode. For sparse rewards I would have had to choose a different strategy, but this is not the case here as far as I can tell. This is by no means the optimal solution, but I was a bit pressed for time.


### Reward plot

Here is the reward plot. As you can see, the convergence was not unstable as in the scores going up, only to go down. The rise of the score was gradual, but also reasonably quick. Once the average level over 20 agents of 30.0 was reached, it stayed stable for the required 100 episodes.

![reward plot](./convergence.png)


### Ideas for future work

DDPG is by no means the only approach to deep reinforcement learning. If given more time, I would have liked to experiment with TNPG, RWR, REPS, TRPO, CEM, CMA-ES and other algorithms, which are compared [here](https://arxiv.org/abs/1604.06778). I could have also tried out PPO for continuous action spaces (see [here](https://blog.openai.com/openai-baselines-ppo/)). There are also even more recent approaches such as [D4PG](https://openreview.net/pdf?id=SyZipzbCb) worth exploring.
